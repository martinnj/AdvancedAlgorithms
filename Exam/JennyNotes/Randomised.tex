\section{Randomised Algorithms}
OMG!!! This you can do! Even in danglish

\subsection{Randomised Quicksort}
Vi vil sortere sættet $S$ på $n$ numre. Hvis vi kan finde et $y$ af $S$, sådan at halvdelen er mindre end $y$, så kan vi dele sættet i 2 bortset fra $y$, sådan at $S_1$ indeholder alle elementer, der er $< y$ og $S_2$ resten. Så sorterer vi rekursivt $S_1$ og $S_2$ og outputter elementerne af $S_1$ sorteret stigende, efterfuldt af $y$ og så elementerne af $S_2$, også i stigende rækkefølge. Hvis vi kunne finde $y$ i $cn$ trin for en eller anden konstant $c$m ville vi kunne partionere $S \setminus \{y\}$ til $S_1$ og $S_2$ i $n - 1$ tilsvarende trin ved at sammenligne hvert element af $S$ med $y$ - og dermed ville det totale antal trin være givet af følgende rekurrens: 

\begin{align}
	T(n) \leq 2T(\frac{n}{2}) + (c + 1)n
\end{align}

hvor $T(k)$ repræsenterer tiden det tager for denne metode at sortere $k$ input. Ved direkte substitution kan vi verificere, at $T(n) \leq c'n \lg n$ for en konstant $c'$. \\

Der er en vis udfordring i, at dele $S_1$ og $S_2$ i to lige store dele. Her prøver vi at gøre det med tilfældighed, og ender med algoritmen: RandQS = Randomised Quicksort\footnote{RandQS er et eksempel på en randomiseret algoritme - altså en algoritme, der foretager et tilfældigt valg på et tidspunkt i sin eksekvering - her i step 1}. \\

\textbf{Køretiden for RandQS kan analyseres til følgende:} \\
Køretiden for randomiseret Quicksort er analyseret ud fra antallet af sammenligninger. Vi vil gerne finde the forventede antal af sammenligninger. \\

For $1 \leq i \leq n$, lad $S_{(i)}$ angive elementet af rang $i$ (det $i$te mindste element) i sæt $S$. Således er $S_{(1)}$ det mindste element og $S_{(n)}$ det største. Definér den tilfældige variabel $X_{ij}$ til at antage værdien $1$ hvis $S_{(i)}$ og $S_{(j)}$ er sammenlignet, og værdien $0$ ellers.\\

Det totale antal sammenligninger er 
\[
	\sum_{i=1}^n \sum_{j>i} X_{ij}
\]
Det forventede antal sammenligninger er (by Linearity of Expectation)
\[
	\mathbb{E}[\sum_{i=1}^n \sum_{j>i} X_{ij}] = \sum_{i=1}^n \sum_{j>i} \mathbb{E}[X_{ij}]
\]

Så lader vi $p_{ij}$ angive sansynligheden for at $S_{(i)}$ og $S_{(j)}$ bliver sammenlignet i en udførelse. $X_{ij}$ kan antage værdierne $0$ og $1$, så:
\[
	\mathbb{E}[X_{ij}] = p_{ij} \times 1 + (1 - p_{ij}) \times 0 = p_{ij}
\]

For at bestemme $p_{ij}$, kigger vi på udførelsen af RandQS som et binært søgetræ $T$, hvor hver node er et særskilt element af $S$. Roden i træet er pivoten $y$, som vi valgte indledningsvist. Venstre subtræ indeholder elementerne i $S_1$ og højre subtræ $S_2$. Strukturen bestemmes rekursivt ved sekventielle kørsler af RandQS. Man sammenligner roden af træet $y$ med elementerne i henholdsvis $S_1$ og $S_2$, men elementerne i $S_1$ og $S_2$ sammenlignes ikke med hinanden. \\

For at udregne $p_{ij}$ laver vi to observationer:

\begin{enumerate}
	\item Der er en sammenligning mellem $S_{(i)}$ og $S_{(j)}$, hvis og kun hvis $S_{(i)}$ eller $S_{(j)}$ 
	fremkommer i permutationen $\pi$ tidligere end noget element $S_{(l)}$ sådan at $i < l < j$. Intuitively, 
	they are only compared if there is some pivot node $l$ that puts $i$ in $S_1$ and $j$ in $S_2$. To see 
	this, let $S_{(k)}$ be the earliest in $\pi$ from among all elements of rank between $i$ and $j$. If $k \notin \{i,j\}$
	then $S_{(i)}$ will belong to the left sub-tree of $S_{(k)}$ while $S_{(j)}$ will belong to the right sub-tree
	of $S_{(k)}$, implying that there is no comparison between $S_{(i)}$ and $S_{(j)}$. Conversely, if
	$k \in \{i,j\}$ then it must be that there is a parent-child relationship between $S_{(i)}$ and $S_{(j)}$
	implying that the two are compared.

	\item Any of the elements $S_{(i)}, S_{(i+1)}, \hdots, S_{(j)}$ is equally likely to be chosen as the pivot and
	hence appear first in $\pi$. Thus, the probability that the first element is either $S_{(i)}$ or $S_{(j)}$ is
	exactly $2 / (j - i + 1)$.
\end{enumerate}

This establishes that $p_{ij} = 2/(j - i + 1)$. The expected number of comparisons is thus:
%
\begin{align*}
	\sum_{i=1}^n \sum_{j>i} p_{ij} &= \sum_{i=1}^n \sum_{j>i} \frac{2}{j-i+1} \\
	&\leq \sum_{i=1}^n \sum_{k=1}^{n-i+1} \frac{2}{k} \\
	&\leq 2\sum_{i=1}^n \sum_{k=1}^{n} \frac{1}{k}
\end{align*}
%
It follows that the expected number of comparisons is bounded by $2nH_n$, where $H_n$ is the \textit{nth}
Harmonic number, defined by $H_n = \sum_{k=1}^n 1/k$. We know that $H_n \sim ln\ n + \Theta(1)$, making the expected running time of RandQS $O(n\ lg\ n)$

\subsection{Las Vegas \& Monte-Carlo}
An algorithm that \textit{always} gives the correct solution, but whose running time is variable or perhaps even
unbounded, is a \textbf{Las Vegas} algorithm. In contrast, an algorithm that may sometimes produce an incorrect
solution (although we can often bound the probability of this), but we know will always terminate, is a
\textbf{Monte-Carlo} algorithm. One useful property of a Monte-Carlo algorithm is that we can reduce the risk of
failure to be arbitrarily small by running the experiment repeatedly with
independent random choices each time.

For decision problems (yes or no problems), there are two kinds of Monte-Carlo
algorithms: those with one-sided error and those with two-sided error. A
Monte-Carlo algorithm is said to have a two-sided error if there is a non-zero
probability that it errs when it outputs either \texttt{YES} or \texttt{NO}. It is said
to have one-sided error if the probability that it errs is zero for at least one of the
possible outputs (\texttt{YES} / \texttt{NO}).

By definition, a Las Vegas algorithm is a Monte-Carlo algorithm with error
probability $0$. We say that a Las Vegas algorithm is an \textit{efficient}
Las Vegas algorithm if on any input its expected running time is bounded by a
polynomial function of the input size. Similarly, we say that a Monte-Carlo
algorithm is an \textit{efficient} Monte-Carlo algorithm if on any input its
worst-case running time is bounded by a polynomial function of the input size.

\subsubsection{Markov \& Chebyshev's inequalities}
\begin{description}
\item[Markov inequality] Let $Y$ be a random variable assuming only non-negative
  values. Then for all $t \in \mathbb{R}^+$,
  \[
    \text{Pr}[Y \geq t] \leq \frac{\text{E[Y]}}{t}.
  \]
  Equivalently,
  \[
    \text{Pr}[Y \geq k\text{E}[Y]] \leq \frac{1}{k}.
  \]
\item[Proof] Define a function $f(y)$:
  \[
   f(y) = \begin{cases}
     1 & \text{iff } y \geq t\\
     0 & \text{otherwise.}
   \end{cases}
  \]
  Then $\text{Pr}[Y \geq t] = \text{E}[f(y)]$. Since $f(y) \leq y/t$ for all $y$,
  \[
    \text{E}[f(Y)] \leq \text{E}\left [\frac{Y}{t} \right ] = \frac{\text{E}[Y]}{t},
  \]
  and the theorem follows.
\end{description}

\begin{description}
\item[Chebyshevs inequality] Let $X$ be a random variable with expectation
  $\mu_X$ and a standard deviation of $\sigma_X$. Then for any $t \in
  \mathbb{R}^+$,
  \[
    \text{Pr}[|X - \mu_X| \geq t\sigma_X] \leq \frac{1}{t^2}
  \]
\item[Proof] Note that
  \[
    \text{Pr}[|X - \mu_X| \geq t\sigma_X] = \text{Pr}[(X - \mu_X)^2 \geq
    t^2\sigma_X^2]
  \]
  The random variable $Y = (X - \mu_X)^2$ has expectation $\sigma_X^2$, and
  applying the Markov inequality to $Y$ bounds this probability from above by
  $1/t^2$. 
\end{description}

\subsection{Disposition}
Forslag - bygget på Martin G's noter - måske er Søren D's bedre...

\begin{enumerate}
	\item Randomised Quicksort
	\item Las Vegas, Monte Carlo
	\item 1-sided vs. 2-sided
	\item Bounded Running Times
	\item Markov \& Chebyshev's Inequalities
	\item Randomised Selection
\end{enumerate}